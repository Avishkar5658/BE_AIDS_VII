{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673ce745-5ee6-482b-abd9-58ed82745901",
   "metadata": {},
   "source": [
    "'''NAME: Aher Swami Sandip\n",
    "ROLL NO. 01\n",
    "COURSE: AI&DS\n",
    "CLASS: BE\n",
    "SUB:Computer Laboratory-II (Information Retrival)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ffa271-85e5-481d-9e52-8ef413dd56e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building web graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swami\\AppData\\Local\\Temp\\ipykernel_13396\\244424162.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(response.text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing PageRank...\n",
      "\n",
      "PageRank Results:\n",
      "https://en.wikipedia.org/wiki/Web_crawler: 0.07500\n",
      "https://en.wikipedia.org/wiki/PageRank: 0.07500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "\n",
    "# Step 1: Crawling - Collect links from a page\n",
    "def get_links_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            if href.startswith(\"http\"):\n",
    "                links.add(href)\n",
    "        return links\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "# Step 2: Build a directed graph from a list of URLs\n",
    "def build_web_graph(seed_urls, max_depth=1):\n",
    "    graph = {}\n",
    "    visited = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth or url in visited:\n",
    "            return\n",
    "        visited.add(url)\n",
    "        links = get_links_from_url(url)\n",
    "        graph[url] = links\n",
    "        for link in links:\n",
    "            crawl(link, depth + 1)\n",
    "\n",
    "    for seed in seed_urls:\n",
    "        crawl(seed, 0)\n",
    "\n",
    "    return graph\n",
    "\n",
    "# Step 3: Compute PageRank using simplified algorithm\n",
    "def compute_pagerank(graph, damping=0.85, max_iterations=100, tol=1e-6):\n",
    "    pages = list(graph.keys())\n",
    "    n = len(pages)\n",
    "    ranks = {page: 1.0 / n for page in pages}\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        new_ranks = {}\n",
    "        for page in pages:\n",
    "            incoming_links = [p for p in pages if page in graph[p]]\n",
    "            rank_sum = sum(ranks[p] / len(graph[p]) for p in incoming_links if graph[p])\n",
    "            new_ranks[page] = (1 - damping) / n + damping * rank_sum\n",
    "\n",
    "        # Check convergence\n",
    "        delta = sum(abs(new_ranks[page] - ranks[page]) for page in pages)\n",
    "        ranks = new_ranks\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    return ranks\n",
    "\n",
    "# Step 4: Run everything together\n",
    "if __name__ == \"__main__\":\n",
    "    seed_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Web_crawler\", \n",
    "        \"https://en.wikipedia.org/wiki/PageRank\"\n",
    "    ]\n",
    "\n",
    "    print(\"Building web graph...\")\n",
    "    web_graph = build_web_graph(seed_urls, max_depth=1)\n",
    "\n",
    "    print(\"Computing PageRank...\")\n",
    "    pagerank_scores = compute_pagerank(web_graph)\n",
    "\n",
    "    print(\"\\nPageRank Results:\")\n",
    "    for page, score in sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{page}: {score:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286d0ff-5d36-41a7-a6d4-4f9468b71be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
